{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254e4920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI embedding model initialized successfully!\n",
      "Table 'document_chunks' created successfully!\n",
      "Text extraction utilities initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "from PyPDF2 import PdfReader\n",
    "from pdfminer.high_level import extract_text as pdfminer_extract_text\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import lancedb\n",
    "import pyarrow as pa\n",
    "from tabula import read_pdf\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load OpenAI API key from environment variables\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Function to generate embeddings using OpenAI\n",
    "def generate_openai_embedding(text):\n",
    "    \"\"\"Generates embeddings using OpenAI's embedding model.\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=text,\n",
    "    )\n",
    "    # Use dot notation to access the response attributes\n",
    "    return response.data[0].embedding\n",
    "\n",
    "print(\"OpenAI embedding model initialized successfully!\")\n",
    "\n",
    "# Initialize LanceDB\n",
    "db = lancedb.connect(\"./lancedb_vectors\")\n",
    "schema = pa.schema([\n",
    "    (\"id\", pa.string()),\n",
    "    (\"text\", pa.string()),\n",
    "    (\"embedding\", pa.list_(pa.float32(), list_size=1536)),  \n",
    "])\n",
    "if \"document_chunks\" in db.table_names():\n",
    "    table = db.open_table(\"document_chunks\")\n",
    "else:\n",
    "    table = db.create_table(\"document_chunks\", schema=schema, mode=\"overwrite\")\n",
    "    print(\"Table 'document_chunks' created successfully!\")\n",
    "\n",
    "# Function to extract text from PDFs using multiple libraries\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "\n",
    "    # Method 1: PyMuPDF\n",
    "    try:\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            for page in pdf:\n",
    "                text += page.get_text(\"text\")\n",
    "    except Exception as e:\n",
    "        print(f\"[PyMuPDF Error] {file_path}: {e}\")\n",
    "\n",
    "    # Method 2: PDFPlumber\n",
    "    if not text.strip():\n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text += page.extract_text() or \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"[PDFPlumber Error] {file_path}: {e}\")\n",
    "\n",
    "    # Method 3: PyPDF2\n",
    "    if not text.strip():\n",
    "        try:\n",
    "            reader = PdfReader(file_path)\n",
    "            text = \"\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "        except Exception as e:\n",
    "            print(f\"[PyPDF2 Error] {file_path}: {e}\")\n",
    "\n",
    "    # Method 4: PDFMiner\n",
    "    if not text.strip():\n",
    "        try:\n",
    "            text = pdfminer_extract_text(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[PDFMiner Error] {file_path}: {e}\")\n",
    "\n",
    "    # Method 5: Tabula-py for extracting tables\n",
    "    if not text.strip():\n",
    "        try:\n",
    "            tables = read_pdf(file_path, pages='all', multiple_tables=True, pandas_options={\"header\": None})\n",
    "            text = \"\\n\".join(df.to_string(index=False) for df in tables)\n",
    "        except Exception as e:\n",
    "            print(f\"[Tabula-py Error] {file_path}: {e}\")\n",
    "\n",
    "    # Fallback: OCR using PyTesseract\n",
    "    if not text.strip():\n",
    "        try:\n",
    "            text = pytesseract.image_to_string(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[OCR Error] {file_path}: {e}\")\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Function for chunking text dynamically\n",
    "def chunk_text(text, chunk_size=800, chunk_overlap=200):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# Function to filter non-informative chunks\n",
    "def filter_chunks(chunks):\n",
    "    return [chunk for chunk in chunks if len(chunk.strip()) > 30]\n",
    "\n",
    "print(\"Text extraction utilities initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "777a45b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks for 3M_2021_10K.pdf: 100%|██████████| 1035/1035 [08:32<00:00,  2.02it/s]\n",
      "Processing PDFs:  11%|█         | 1/9 [08:33<1:08:31, 513.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1035 records for 3M_2021_10K.pdf to LanceDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks for 3M_2022_10K.pdf: 100%|██████████| 1607/1607 [13:24<00:00,  2.00it/s]\n",
      "Processing PDFs:  22%|██▏       | 2/9 [21:58<1:19:53, 684.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1607 records for 3M_2022_10K.pdf to LanceDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks for 3M_2023Q2_10Q.pdf: 100%|██████████| 633/633 [04:56<00:00,  2.13it/s]\n",
      "Processing PDFs:  33%|███▎      | 3/9 [26:55<50:45, 507.59s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 633 records for 3M_2023Q2_10Q.pdf to LanceDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks for 3M_2015_10K.pdf: 100%|██████████| 1014/1014 [08:03<00:00,  2.10it/s]\n",
      "Processing PDFs:  44%|████▍     | 4/9 [34:58<41:30, 498.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1014 records for 3M_2015_10K.pdf to LanceDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks for 3M_2019_10K.pdf: 100%|██████████| 1162/1162 [09:34<00:00,  2.02it/s]\n",
      "Processing PDFs:  56%|█████▌    | 5/9 [44:32<35:02, 525.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1162 records for 3M_2019_10K.pdf to LanceDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks for 3M_2017_10K.pdf: 100%|██████████| 1045/1045 [08:15<00:00,  2.11it/s]\n",
      "Processing PDFs:  67%|██████▋   | 6/9 [52:48<25:46, 515.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1045 records for 3M_2017_10K.pdf to LanceDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks for 3M_2016_10K.pdf: 100%|██████████| 1212/1212 [09:48<00:00,  2.06it/s]\n",
      "Processing PDFs:  78%|███████▊  | 7/9 [1:02:36<17:58, 539.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1212 records for 3M_2016_10K.pdf to LanceDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks for 3M_2018_10K.pdf: 100%|██████████| 1034/1034 [08:12<00:00,  2.10it/s]\n",
      "Processing PDFs:  89%|████████▉ | 8/9 [1:10:49<08:44, 524.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1034 records for 3M_2018_10K.pdf to LanceDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding chunks for 3M_2020_10K.pdf: 100%|██████████| 1022/1022 [08:03<00:00,  2.11it/s]\n",
      "Processing PDFs: 100%|██████████| 9/9 [1:18:52<00:00, 525.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1022 records for 3M_2020_10K.pdf to LanceDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Process a single document and extract chunks\n",
    "def process_document(file_path):\n",
    "    text = extract_text_from_pdf(file_path)\n",
    "    if not text.strip():\n",
    "        return {\"id\": os.path.basename(file_path), \"chunks\": []}\n",
    "    chunks = chunk_text(text)\n",
    "    filtered_chunks = filter_chunks(chunks)\n",
    "    return {\"id\": os.path.basename(file_path), \"chunks\": filtered_chunks}\n",
    "\n",
    "# Generate embeddings for document chunks and add to LanceDB\n",
    "def generate_embeddings_for_chunks(doc):\n",
    "    chunk_texts = doc[\"chunks\"]\n",
    "    embeddings = []\n",
    "    for chunk in tqdm(chunk_texts, desc=f\"Embedding chunks for {doc['id']}\"):\n",
    "        embedding = generate_openai_embedding(chunk)\n",
    "        embeddings.append(embedding)\n",
    "    records = [{\"id\": f\"{doc['id']}_chunk{i}\", \"text\": chunk_texts[i], \"embedding\": embeddings[i]} for i in range(len(chunk_texts))]\n",
    "    try:\n",
    "        table.add(records)\n",
    "        print(f\"Added {len(records)} records for {doc['id']} to LanceDB.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding records to LanceDB for {doc['id']}: {e}\")\n",
    "\n",
    "# Process all PDFs in a directory and store embeddings\n",
    "def process_pdfs(directory_path):\n",
    "    pdf_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith(\".pdf\")]\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_document, file_path) for file_path in pdf_files]\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing PDFs\"):\n",
    "            doc = future.result()\n",
    "            if doc[\"chunks\"]:\n",
    "                generate_embeddings_for_chunks(doc)\n",
    "\n",
    "# Specify the directory containing PDF files\n",
    "directory_path = \"../pdfs1\"  # Replace with the correct directory path\n",
    "process_pdfs(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57cc57eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved to ../data/evaluation_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from rapidfuzz import fuzz  # Using rapidfuzz for improved performance\n",
    "\n",
    "# Load OpenAI API key\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Query LanceDB for relevant chunks\n",
    "def query_documents(question, top_k=20):\n",
    "    question_embedding = generate_openai_embedding(question)  # Use OpenAI for embeddings\n",
    "    results = table.search(query=question_embedding, vector_column_name=\"embedding\").limit(top_k).to_pandas()\n",
    "    return results[\"text\"].tolist()\n",
    "\n",
    "# Generate a response using GPT\n",
    "def generate_response(question, chunks):\n",
    "    context = \"\\n\\n\".join(chunks)\n",
    "    instructions = (\n",
    "       \"You are a highly skilled financial analyst specializing in corporate financial reports. \"\n",
    "        \"Your goal is to provide precise and concise responses to multi-hop questions about 10-K \"\n",
    "        \"filings. You have multiple pieces of context from which you can summarize and integrate facts. \"\n",
    "        \"Instructions:\\n\"\n",
    "        \"1. Combine relevant data from all context blocks.\\n\"\n",
    "        \"2. Provide numeric results and references when needed.\\n\"\n",
    "        \"3. If the context is insufficient or contradictory, state so.\\n\"\n",
    "        \"4. Be concise and directly address the multi-part question.\\n\"\n",
    "        \"5. Do not guess beyond the provided context.\"\n",
    "\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instructions},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {question}\"},\n",
    "        ],\n",
    "        model=\"gpt-4\",\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Filter questions for 3M\n",
    "def filter_3m_questions(json_file_path):\n",
    "    df = pd.read_json(json_file_path, lines=True)\n",
    "    return df[df[\"doc_name\"].str.contains(\"3M\", case=False, na=False)]\n",
    "\n",
    "# Evaluate model against filtered 3M questions\n",
    "def evaluate_model(json_file_path, output_file_path):\n",
    "    df = filter_3m_questions(json_file_path)\n",
    "    results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        question = row[\"question\"]\n",
    "        correct_answer = row[\"answer\"]\n",
    "        chunks = query_documents(question, top_k=20)\n",
    "        model_answer = generate_response(question, chunks)\n",
    "\n",
    "        # Evaluate answer accuracy using fuzzy matching\n",
    "        similarity = fuzz.partial_ratio(correct_answer.lower(), model_answer.lower())\n",
    "        evaluation = \"Correct\" if similarity > 85 else \"Incorrect\"\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"model_answer\": model_answer,\n",
    "            \"evaluation\": evaluation,\n",
    "            \"similarity_score\": similarity\n",
    "        })\n",
    "\n",
    "    # Save results to JSONL file\n",
    "    pd.DataFrame(results).to_json(output_file_path, orient=\"records\", lines=True)\n",
    "    print(f\"Evaluation results saved to {output_file_path}\")\n",
    "\n",
    "# Evaluate against the JSON file\n",
    "json_file_path = \"../data/financebench_open_source.jsonl\"\n",
    "output_file_path = \"../data/evaluation_results.jsonl\"\n",
    "evaluate_model(json_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff67435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
